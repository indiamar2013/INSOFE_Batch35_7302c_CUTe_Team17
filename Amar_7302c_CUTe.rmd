---
title: "7302c CUTe"
author: "Amar Rao"
date: "December 14, 2017"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
    theme: united
    highlight: tang
    fig_width: 7
    fig_height: 6
    fig_caption: true
    code_folding: hide

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

#clear all environment and session variables
rm(list = ls(all = TRUE))


```


Libraries
```{r}

library(knitr)
library(tidyverse)
library(lubridate)
library(caret)
library(DMwR)
library(forecast)
library(lubridate)
library(imputeTS)
library(TTR)
library(graphics)
library(zoo)
```

## Reading data

* set working directory
```{r}
setwd('c:/Users/212629693/Documents/Personal/ds-classes/INSOFE_Batch35_7302c_CUTe_Team17')

```

### Load the data set

```{r}
fin_data_orig <- read.csv('Datasets/train_data.csv', header = TRUE, sep = ',', na.strings = "", colClasses = "double", numerals = "no.loss")
# str(fin_data_orig, list.len = ncol(fin_data_orig))
# head(fin_data_orig)
```

* We know y2 is a binary column. converting that to factor.

```{r}
cat_attrs <- c('y2')
num_attrs <- setdiff(colnames(fin_data_orig), cat_attrs)

fin_data_orig[cat_attrs] <- data.frame(sapply(fin_data_orig[cat_attrs], as.factor))
#str(fin_data_orig, list.len = ncol(fin_data_orig))
#head(fin_data_orig, 25)
```

```{r}
#summary(fin_data_orig)
```


###Observations:

* There're 109 regressors and 2 target variables (y1 and y2) and 1769 observations
* Data appears to already have been scaled and standardized. 
* There are a few empty columns (ie all rows are blank in these columns )
* Some values are missing in other columns
* Timestamp starts as a day number at 14 and goes up to 1782 days (1769 days in total)



## Preprocessing
### Empty Columns

  * Following columns are completely blank (column with all NA values) in the data frame.

```{r}
emptycols <- colnames(fin_data_orig[,sapply(fin_data_orig, function(x) { all(is.na(x))})])
emptycols
```

*Removing these columns

```{r}
fin_data <- fin_data_orig[, !colnames(fin_data_orig) %in% emptycols]
#str(fin_data)
```

### Handle NAs

* Checking how spread the NAs are in other columns and selecting columns that have more than 25% NAs to remove
```{r}
colswithna <- sort(colSums(is.na(fin_data)), decreasing = TRUE)

nacolstoremove <- colswithna[colswithna > nrow(fin_data)*0.03]
nacolstoremove
```

####So removing those columns. Will use knnImputation for the rest once I do the test-train split.

```{r}
fin_data <- fin_data[, !colnames(fin_data) %in% names(nacolstoremove)]
#str(fin_data)
```


* checking if there're any sparse columns (ie columns with very large number of zeros)

```{r}
sparsecols <- sort(colSums(fin_data[,!colnames(fin_data) %in% c("y1", "y2")] == 0), decreasing = TRUE)
sparsecols
colstoremove <- sparsecols[sparsecols > round(nrow(fin_data) * 0.25)]
colstoremove
```

* since these have very large number of zeros, they cannot have any meaningful impact on the target variables. So removing them.

```{r}
fin_data <- fin_data[, !colnames(fin_data) %in% names(colstoremove)]

#str(fin_data)
```
### convert timestamp to date
* The timestamps are given as day numbers starting with 14. We have observations for 1769 days (approx 4.8 years of data)



### Train-test split

```{r}
#Since this is chronological data, we cannot do random sampling.
# Setting the first 90% to train and last 10% to test
train_rows <- 1:round(nrow(fin_data)*0.9)

fin_train <- fin_data[train_rows, ]

fin_test <- fin_data[-train_rows, ]

```

###Imputation

```{r}

library(RANN)
set.seed(1234)
preproc_preds <- preProcess(x = subset(fin_train, select = -c(y1, y2)), method = c("range", "knnImpute"))

fin_train <- predict(object = preproc_preds, fin_train)
fin_test <- predict(object = preproc_preds, fin_test)
sum(is.na(fin_train))
sum(is.na(fin_test))

```


## scaling

```{r}

scale_preds <- preProcess(x = subset(fin_train, select = -c(y1, y2), method = c("center", "scale")))
fin_train <- predict(object = scale_preds, fin_train)
fin_test <- predict(object = scale_preds, fin_test)

```

## Assessing fit for Timeseries Forecasting
* Need to check if y1 exhibits strong trend and some seasonality (optionally). If so, we can proceed with timeseries forecasting. We will assume 252 trading days in a year, so we have to start the series from 2010 Jan 1



```{r}
y1ts <- ts(fin_train$y1, start = c(2010, 1,1), frequency = 252)
plot(y1ts,
     type="l",
     lwd=2,
     col="blue",
     xlab="Daily",
     ylab="Change",
     main="Time series plot for stock - for target variable y1")
```

```{r}
y1decomp <- decompose(x = y1ts)
plot(y1decomp)
```

###Observations:
* We see that there's a fairly steady trend moving within a small range
* Seasonality at daily frequency seem to exist. 

###Will first use this without any transformations to see if ARIMA model exists
### Arima requires xargs to be of rank = ncols, so we need to ensure that's the case

```{r}
library(Matrix)
fin_x_reg <- subset(fin_train, select = -c(y1, y2))

nrow(fin_x_reg)
ncol(fin_x_reg)
rmtx <- rankMatrix(fin_x_reg)
rmtx[1]

```


* Rank of the matrix (87) is less than the number of columns. This will cause errors running Arima or auto.arima. So need to eliminate that.

```{r}


constant_cols <- names(fin_train[, sapply(fin_train, function(v) var(v, na.rm=TRUE)==0)])
constant_cols

```

* Will exclude these columns for further analysis

```{r}
fin_train <- fin_train[, !colnames(fin_train) %in% constant_cols]
fin_test <- fin_test[, !colnames(fin_test) %in% constant_cols]

##also updating the original set
fin_data <- fin_data[, !colnames(fin_data) %in% constant_cols]


#confirming if that fixes the rank issue
rmtx <- rankMatrix(as.matrix(subset(fin_train, select = -c(y1, y2))))
rmtx[1]
ncol(fin_train)
```

* now creating separate dataframes for y1 and y2

```{r}

y1_train <- subset(fin_train, select = -y2)
y1_test <- subset(fin_test, select = -y2)
y2_train <- subset(fin_train, select = -y1)
y2_test <- subset(fin_test, select = -y1)


```


##MODEL BUILDING

##ARIMA with no transformation 
*ACF

```{r}

acf(y1ts)


```

ACF shows that there's 4 significant lags so we can use non-seasonal q = 4

*PACF

```{r}
pacf(y1ts)

```

PACF indicates 5 significant lags, so we can use non seasonal p = 4

* Check if we need to difference

```{r}
ndiffs(y1ts)
nsdiffs(y1ts)
```

* No need to difference

###First we will build a basic Arima(2,0,4) model without any xregs
* Premise here is that previous values of y1 are strongest predictors for future y1

```{r}
arima1 <- Arima(y = y1ts, order = c(6,0,4))
arima1
```

*Predictions with arima1

```{r}
arima1preds <- forecast(object = arima1, h = nrow(y1_test))

plot(arima1preds)
```


* Performance Metrics for arima1

```{r}
accuracy(arima1preds, y1_test$y1)

```

*Getting a MAPE of 215.59 with training set and 126.37 with test set.

##Now Arima with xregs (all columns)

```{r}
arima2 <- Arima(y = y1ts, order= c(6,0,4), xreg = subset(y1_train, select = -y1))
arima2
arima2preds <- forecast(object = arima2, h = nrow(y1_test), xreg = subset(y1_train, select = -y1))

plot(arima2preds)
```


```{r}
accuracy(arima2preds, y1_test$y1)

```

* MAPE jumped to 450.21 vs training and 218.21 against test.


###So Arima with all xregs in the dataset doesn't work. We will have to find out which columns to include.
For that, using StepAIC. IN that process, we will also evaluate the performance of OLS.

##StepAIC
###First with OLS
```{r}

fin_train$timestamp <- NULL
y1_train$timestamp <- NULL

base_linreg <- lm(y1 ~ ., data = y1_train)

summary(base_linreg)
par(mfrow=c(2,2))
plot(base_linreg)
par(mfrow=c(2,2))

```


* This model suffers from a very low Adjusted Rsquared and also heteroscadasticity
* First we will see if heteroscadasticity can be resolved 

```{r}
#function to help set the daily price.
setdailyprice <- function(daily_price) {
  retprice <- daily_price
  for(i in 1:length(daily_price)) {
    if (i == 1) {
      retprice[i] = 100 + (100 *daily_price[i])
    } else {
      retprice[i] = retprice[i-1]+(100 * daily_price[i])
    }

  }
  return(retprice)
}



```

```{r}
new_y1_train <- y1_train
new_y1_train$newy1 <- cbind(setdailyprice(y1_train$y1))
new_y1_train <- subset(new_y1_train, select = -c(y1))
new_y1_test <- y1_test
new_y1_test$newy1 <- cbind(setdailyprice(new_y1_test$y1))
new_y1_test <- subset(new_y1_test, select = -c(y1))

```

```{r}

new_lm_mdl <- lm(newy1 ~ ., data = new_y1_train)
summary(new_lm_mdl)
par(mfrow = c(2,2))
plot(new_lm_mdl)
par(mfrow = c(1,1))

```


###Predictions with OLS and its performance

```{r}

ols_all_preds <- predict(new_lm_mdl, new_y1_test)

print('Error metrics for Train data')
print(regr.eval(ols_all_preds, new_y1_train$newy1))
print("")
print('Error metrics for Test data')
print(regr.eval(ols_all_preds, new_y1_test$newy1))
```

###OLS with all variables is giving a MAPE of 15.64% with Training data and 1.05% with Test Data!!!

#let's see if we can improve things by removing row 1208

```{r}
new_y1_train <- new_y1_train[-1208,]


log_lm_mdl <- lm(newy1 ~ ., data = new_y1_train)
par(mfrow = c(2,2))
plot(log_lm_mdl)
par(mfrow = c(1,1))
```


```{r}
ols_all_preds <- predict(log_lm_mdl, new_y1_test)

print('Error metrics for Train data')
print(regr.eval(ols_all_preds, new_y1_train$newy1))
print("")
print('Error metrics for Test data')
print(regr.eval(ols_all_preds, new_y1_test$newy1))
```



* Let's see if StepAIC can do better

```{r}
library(MASS)
aicoptions <- stepAIC(log_lm_mdl, direction = "both", trace = FALSE)
summary(aicoptions)
```


####Based on stepAIC, will use only the following regressors:

    
```{r}

myfactors <- colnames(aicoptions$model)

ncol(aicoptions$model)
myfactors <- myfactors[-1]

#reformulate(termlabels = listoffactors, response = 'y')
lmformula <- reformulate(termlabels = myfactors, response = "newy1")

aic_rec_mdl <- lm(formula = lmformula, data = new_y1_train)


summary(aic_rec_mdl)
par(mfrow=c(2,2))
plot(aic_rec_mdl)
par(mfrow=c(1,1))

require(car)
var_vifs <- sort(vif(aic_rec_mdl), decreasing=TRUE)
collinearity_vars <- var_vifs[var_vifs > 10]
collinearity_vars
length(collinearity_vars)

require(ggcorrplot)

ggcorrplot(cor(new_y1_train[,colnames(new_y1_train) %in% names(collinearity_vars)]), hc.order = TRUE, type = "upper", insig = "blank")
cors <- data.frame(cor(new_y1_train[,colnames(new_y1_train) %in% names(collinearity_vars)]))


f_cols <- cors[sapply(colnames(cors), function(x) {grepl('f_', x,  fixed=TRUE)}),
     sapply(colnames(cors), function(x) {grepl('f_', x,  fixed=TRUE)})]
ggcorrplot(f_cols, type = "upper", insig = "blank")

t_cols <- cors[sapply(colnames(cors), function(x) {grepl('t_', x,  fixed=TRUE)}),
     sapply(colnames(cors), function(x) {grepl('t_', x,  fixed=TRUE)})]
ggcorrplot(t_cols, type = "upper", insig = "blank")

d_cols <- cors[sapply(colnames(cors), function(x) {grepl('d_', x,  fixed=TRUE)}),
     sapply(colnames(cors), function(x) {grepl('d_', x,  fixed=TRUE)})]
ggcorrplot(d_cols, type = "upper", insig = "blank")


corrplot::corrplot(cor(new_y1_train[,colnames(new_y1_train) %in% names(collinearity_vars)]))
aic_rec_mdl$coefficients
```

```{r}

low_collinear_cols <- c('d_0', "d_1", "d_2", "d_3",
                        "f_0", "f_2", "f_5","f_7",
                        "f_8", "f_9", "f_10", "f_12", 
                        "f_14", "f_15", "f_16", "f_19", 
                        "f_22", "f_23", "f_24", "f_29",
                        "f_30", "f_32", "f_33", "f_37",
                        "f_39", "f_40", "f_41", "f_42",
                        "f_43", "f_44", "f_51", "f_52",
                        "f_53", "f_54", "f_58", "f_60",
                        "t_3", "t_6", "t_11", "t_14",
                        "t_21", "t_22", "t_27", "t_35",
                        "t_38", "t_39")

lmformula <- reformulate(termlabels = low_collinear_cols, response = "newy1")
lmformula
lm_mdl2 <- lm(lmformula, data = new_y1_train)
summary(lm_mdl2)
plot(lm_mdl2)
mdl2_preds <- predict(lm_mdl2, y1_test)

print('Error metrics for Train data')
print(regr.eval(mdl2_preds, new_y1_train$newy1))
print("")
print('Error metrics for Test data')
print(regr.eval(mdl2_preds, new_y1_test$newy1))

new_aic <- stepAIC(object = lm_mdl2, direction = "both")

```

* WIth StepAIC recommended model MAPE on training went up to 3.2 and on test went up to 1.09

```{r}

lm_mdl3 <- lm(newy1 ~ d_1 + d_2 + d_3 + f_0 + f_2 + f_5 + f_7 + f_8 + f_9 + 
    f_10 + f_12 + f_14 + f_15 + f_16 + f_19 + f_22 + f_24 + f_29 + 
    f_30 + f_32 + f_33 + f_37 + f_39 + f_40 + f_41 + f_42 + f_44 + 
    f_51 + f_52 + f_53 + f_54 + f_58 + t_3 + t_6 + t_11 + t_14 + 
    t_21 + t_22 + t_27 + t_35 + t_38 + t_39, data = new_y1_train)
plot(lm_mdl3)
summary(lm_mdl3)
```

```{r}

mdl3_preds <- predict(lm_mdl3, y1_test)

print('Error metrics for Train data')
print(regr.eval(mdl3_preds, new_y1_train$newy1))
print("")
print('Error metrics for Test data')
print(regr.eval(mdl3_preds, new_y1_test$newy1))

```


##Now we will try Arima with regressors based on StepAIC recommended regressors

```{r}
# start_row <- nrow(new_y1_train)-(252*2)
# end_row <- nrow(new_y1_train)
# recent_train <- new_y1_train[start_row : end_row, ]
mdloptions <- colnames(lm_mdl3$model)
mdloptions <- mdloptions[!mdloptions %in% c('d_3', 'f_5', 'f_24', 'f_58', 't_11', 't_39')]
recent_train <- new_y1_train[,colnames(new_y1_train) %in% mdloptions]
new_y1_ts <- ts(recent_train$newy1, start = c(2010, 1,1), frequency = 365)
acf(new_y1_ts)
pacf(new_y1_ts)
plot(decompose(new_y1_ts))
ndiffs(new_y1_ts)
nsdiffs(new_y1_ts)
new_x_reg <- recent_train[,!colnames(recent_train) %in% 'newy1']
nrow(new_x_reg)
ncol(new_x_reg)
rmtx <- rankMatrix(new_x_reg)
rmtx[1]
constant_cols <- names(new_x_reg[, sapply(new_x_reg, function(v) var(v, na.rm=TRUE)==0)])
constant_cols
str(constant_cols)
length(constant_cols)
new_x_reg <- new_x_reg[, !colnames(new_x_reg) %in% constant_cols]

arima3 <- auto.arima(y = new_y1_ts, xreg = new_x_reg, allowdrift = TRUE)


arima3
arima3_preds <- forecast(object = arima3, h = nrow(new_y1_test), xreg = new_x_reg)
print(accuracy(arima3_preds,new_y1_test$newy1))

plot(arima3_preds)
acf(arima3$residuals)
pacf(arima3$residuals)

arima4 <- Arima(y = new_y1_ts, xreg = new_x_reg, order = c(3,1,4))
arima4
test_x_reg <- new_y1_test[,colnames(new_y1_test) %in% colnames(new_x_reg)]
arima4_preds <- forecast(object = arima4, xreg = test_x_reg)
print(accuracy(arima3_preds,new_y1_test$newy1))
ndiffs(x = new_y1_ts)
plot(arima4_preds)
acf(arima4$residuals)
pacf(arima4$residuals)
plot(residuals(arima4), type='response')

```

* Looks like regressors have made the predictions worse. MAPE is 1064.665 for training set and 354.393 for test set.


ACF and PACF of residuals

```{r}
acf(arima3$residuals)
```
```{r}
pacf(arima3$residuals)
```

#Trying with auto.arima

```{r}

arima4 <- auto.arima(y = y1ts, xreg = new_y1_xreg)


arima4

arima4_preds <- forecast(object = arima4, h = nrow(y1_test), xreg = new_y1_xreg)
print(accuracy(arima4_preds,y1_test$y1))

plot(arima4_preds)

```

Not much improvement in MAPE with AutoArima suggested (3,0,0)

##Seeing the performance of LASSO

###First will try a LASSO model with all variables

```{r}
library(glmnet)
set.seed(1234)

cv_lasso <- cv.glmnet(as.matrix(new_x_reg), as.matrix(new_y1_train$newy1), alpha = 1, type.measure = "deviance", nfolds = 4)
par(mfrow=c(1,2))
plot(cv_lasso)
plot(cv_lasso$glmnet.fit, xvar = 'lambda', label = TRUE)
par(mfrow=c(1,1))


```

* Now using the min lambda, running lasso regression

```{r}
cv_lasso$lambda
cv_lasso$lambda.1se
lasso_1 <- glmnet(as.matrix(new_x_reg), as.matrix(new_y1_train$newy1), family = "gaussian", alpha = 1, lambda = 75)

#63.15664686

lasso_1_test_preds <- predict(lasso_1, as.matrix(test_x_reg))
lasso_1_train_preds <- predict(lasso_1, as.matrix(new_x_reg))
regr.eval(lasso_1_train_preds, new_y1_train$newy1)
regr.eval(lasso_1_test_preds, new_y1_test$newy1)


```

* This gives a MAPE of 9.38 on test data

###Now trying LASSO with the set of variables from StepAIC

```{r}
new_y1_xreg
X_train = as.matrix(new_y1_xreg)

X_test <- as.matrix(subset(y1_test[, colnames(aic_rec_mdl$model)], select = -y1))

cv_lasso_1 <- cv.glmnet(X_train, as.matrix(y1_train$y1), alpha = 1,  nfolds = 4)

par(mfrow=c(1,2))
plot(cv_lasso_1)
plot(cv_lasso_1$glmnet.fit, xvar = 'lambda', label = TRUE)
par(mfrow=c(1,2))



```



```{r}
lasso_2 <- glmnet(X_train, as.matrix(y1_train$y1), family = "gaussian", lambda = cv_lasso_1$lambda.min, alpha = 1)

lasso_2_preds <- predict(lasso_2, X_test)

regr.eval(lasso_2_preds, y1_test$y1)
```

MAPE with this LASSO model gives a mape of 1.10


###refactor below







#ARIMA Revisited
###This is inspired by the discussion with Vamshi Anand (Batch 35). All credit
to him if this works out to be a better model

* The previous models with ARIMA were quite poorly performing. The response variable y1 is a daily percentage change. Hence the trend was quite linear and flat.

*The idea is what if we instead create a new response variable that is actually the price based on the price change. Can we see a trend and seasonality that would in turn give a better model?

* Let's create a new column DailyPrice that is an actual value that is previous day's value * percent change for the day.First day's price is set to 100 as a base.

```{r}
#function to help set the daily price.
setdailyprice <- function(daily_price) {
  retprice <- daily_price
  for(i in 1:length(daily_price)) {
    if (i == 1) {
      retprice[i] = 100
    } else {
      retprice[i] = retprice[i-1]*(1+ daily_price[i])
    }

  }
  return(retprice)
}


```


###Train-Test sets with this new variable

```{r}
fin_data1 <- fin_data

fin_data1$DailyPrice <- setdailyprice(fin_data$y1)

train_rows <- 1:round(nrow(fin_data1)*0.7)

fin_train1 <- fin_data1[train_rows, ]

fin_test1 <- fin_data1[-train_rows, ]

dp_train1 <- subset(fin_train1, select = c(-y1, -y2))
dp_test1 <- subset(fin_test1, select = c(-y1, -y2))


```


* Now we can create a new timeseries using this variable

```{r}

dailyprice_ts <- ts(fin_train1$DailyPrice, start = c(2010, 1,1), frequency = 252)
plot(dailyprice_ts)
```

```{r}
plot(decompose(dailyprice_ts))

```

*Let's see if we need to difference anything

```{r}
print(ndiffs(dailyprice_ts))
print(nsdiffs(dailyprice_ts))
```

We need to add one non-seasonal difference

ACF

```{r}
acf(dailyprice_ts, lag.max = 30)
```

ACF shows that it is a very slow decay and strong connect evey beyond 
PACF

```{r}
pacf(dailyprice_ts, lag.max = 30)
```

Will let auto arima pick the right values

* First just DailyPrice (ie no xregs)
```{r}

arima5 <- auto.arima(y = dailyprice_ts, allowdrift = TRUE)
arima5

```

```{r}

arima5_preds <- forecast(arima5, h = nrow(y1_test))
plot(arima5_preds)
print(accuracy(arima5_preds, dp_test1$DailyPrice))


```

This model did very well with Training set (MAPE of 1.97) but not as good against the test set where the MAPE was 13.09

* How about the Regressors?

```{r}
arima6 <- auto.arima(dailyprice_ts, xreg = new_y1_xreg)
arima6
```

Predictions and Performance
```{r}



arima6_preds <- forecast(arima6, h = nrow(y1_test), xreg = new_y1_xreg)
plot(arima6_preds)
accuracy(arima6_preds, dp_test1$DailyPrice)

```

