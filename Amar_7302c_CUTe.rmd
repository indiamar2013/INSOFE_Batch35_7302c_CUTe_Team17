---
title: "7302c CUTe"
author: "Amar Rao"
date: "December 14, 2017"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
    theme: united
    highlight: tang
    fig_width: 7
    fig_height: 6
    fig_caption: true
    code_folding: hide

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

#clear all environment and session variables
rm(list = ls(all = TRUE))


```


Libraries
```{r}

library(knitr)
library(tidyverse)
library(lubridate)
library(caret)
library(DMwR)
library(forecast)
library(lubridate)
library(imputeTS)
library(TTR)
library(graphics)
library(zoo)
```

## Reading data

* set working directory
```{r}
setwd('C:/Users/212629693/Documents/Personal/ds-classes/INSOFE_Batch35_7302c_CUTe_Team17')

```

### Load the data set

```{r}
fin_data_orig <- read.csv('Datasets/train_data.csv', header = TRUE, sep = ',', na.strings = "", colClasses = "double", numerals = "no.loss")
str(fin_data_orig, list.len = ncol(fin_data_orig))
head(fin_data_orig)
```

* We know y2 is a binary column. converting that to factor.

```{r}
cat_attrs <- c('y2')
num_attrs <- setdiff(colnames(fin_data_orig), cat_attrs)

fin_data_orig[cat_attrs] <- data.frame(sapply(fin_data_orig[cat_attrs], as.factor))
str(fin_data_orig, list.len = ncol(fin_data_orig))
```

```{r}
summary(fin_data_orig)
```


###Observations:

* There're 109 regressors and 2 target variables (y1 and y2) and 1769 observations
* Data appears to already have been scaled and standardized. 
* There are a few empty columns (ie all rows are blank in these columns )
* Some values are missing in other columns
* Timestamp starts as a day number at 14 and goes up to 1782 days (1769 days in total)



## Preprocessing
### Empty Columns

  * Following columns are completely blank (column with all NA values) in the data frame.

```{r}
emptycols <- colnames(fin_data_orig[,sapply(fin_data_orig, function(x) { all(is.na(x))})])
emptycols
```

*Removing these columns

```{r}
fin_data <- fin_data_orig[, sapply(fin_data_orig, function(x) { !all(is.na(x))})]
```

### Handle NAs

* Checking how spread the NAs are in other columns
```{r}
colswithna <- colSums(is.na(fin_data))

sort(colswithna, decreasing = TRUE)

```

#### The following columns have very large number of NAs so imputing those would make the data diluted.

*f_63
*f_27
*f_3
*f_28
*f_31
*f_35
*f_25
*f_17

####So removing those columns. Will use knnImputation for the rest once I do the test-train split.

```{r}
fin_data <- subset(fin_data,select = -c(f_63, f_27, f_3, f_28, f_31, f_35, f_25, f_17))

```

### convert timestamp to date
* The timestamps are given as day numbers starting with 14. We have observations for 1769 days (approx 4.8 years of data)

* Generating dates from 2012 and setting that as the id columns
```{r}
origindate <- as.Date('2011-12-31', format = '%Y-%m-%d')
origindate
fin_data$timestamp = as.Date(fin_data$timestamp, origindate)
rownames(fin_data) <- fin_data$timestamp

#removing timestamp column
fin_data$timestamp <- NULL
head(fin_data)
tail(fin_data)
```

* checking if there're any sparse columns (ie columns with very large number of zeros)

```{r}
sort(colSums(fin_data == 0), decreasing = TRUE)

```

* from the above we can see that t_10, t_13, t_30, t_20 all have very large number of 0s. Checking to see if these are factors

```{r}
tail(fin_data$t_10, 200)
tail(fin_data$t_13, 200)
tail(fin_data$t_30, 200)
tail(fin_data$t_20, 100)
```

* from looking at the data in these tables, it is clear that these are not factors.
* since these have very large number of zeros, they cannot have any meaningful impact on the target variables. So removing them.

```{r}
fin_data <- subset(fin_data, select = -c(t_10, t_13, t_30, t_20))


```


### Train-test split

```{r}
#Since this is chronological data, we cannot do random sampling.
# Setting the first 70% to train and last 30% to test
train_rows <- 1:round(nrow(fin_data)*0.7)

fin_train <- fin_data[train_rows, ]

fin_test <- fin_data[-train_rows, ]

```

###Imputation

```{r}

library(RANN)
set.seed(1234)
preproc_preds <- preProcess(x = subset(fin_train, select = -c(y1, y2)), method = c("knnImpute"))

fin_train <- predict(object = preproc_preds, fin_train)
fin_test <- predict(object = preproc_preds, fin_test)
sum(is.na(fin_train))
sum(is.na(fin_test))

```

## Assessing fit for Timeseries Forecasting
* Need to check if y1 exhibits strong trend and some seasonality (optionally). If so, we can proceed with timeseries forecasting. We will assume 252 trading days in a year, so we have to start the series from 2010 Jan 1



```{r}
y1ts <- ts(fin_train$y1, start = c(2010, 1,1), frequency = 252)
plot(y1ts,
     type="l",
     lwd=2,
     col="blue",
     xlab="Daily",
     ylab="Change",
     main="Time series plot for stock - for target variable y1")
```

```{r}
y1decomp <- decompose(x = y1ts)
plot(y1decomp)
```

###Observations:
* We see that there's a fairly steady trend moving within a small range
* Seasonality at daily frequency seem to exist. 

###Will first use this without any transformations to see if ARIMA model exists
### Arima requires xargs to be of rank = ncols, so we need to ensure that's the case

```{r}
library(Matrix)
fin_x_reg <- subset(fin_train, select = -c(y1, y2))

nrow(fin_x_reg)
ncol(fin_x_reg)
rankMatrix(fin_x_reg)
```


* Rank of the matrix (87) is less than the number of columns. This will cause errors running Arima or auto.arima. So need to eliminate that.

```{r}

constant_cols <- names(fin_train[, sapply(fin_train, function(v) var(v, na.rm=TRUE)==0)])
constant_cols
str(constant_cols)
```

* Will exclude these columns for further analysis

```{r}
fin_train <- fin_train[, !colnames(fin_train) %in% constant_cols]
fin_test <- fin_test[, !colnames(fin_test) %in% constant_cols]

##also updating the original set
fin_data <- fin_data[, !colnames(fin_data) %in% constant_cols]


#confirming if that fixes the rank issue
rankMatrix(as.matrix(subset(fin_train, select = -c(y1, y2))))
```

* now creating separate dataframes for y1 and y2

```{r}

y1_train <- subset(fin_train, select = -y2)
y1_test <- subset(fin_test, select = -y2)
y2_train <- subset(fin_train, select = -y1)
y2_test <- subset(fin_test, select = -y1)


```


##MODEL BUILDING

##ARIMA with no transformation 
*ACF

```{r}

acf(y1ts)


```

ACF shows that there's 4 significant lags so we can use non-seasonal q = 4

*PACF

```{r}
pacf(y1ts)

```

PACF indicates 5 significant lags, so we can use non seasonal p = 5

* Check if we need to difference

```{r}
ndiffs(y1ts)
nsdiffs(y1ts)
```

* No need to difference

###First we will build a basic Arima(5,0,4) model without any xregs
* Premise here is that previous values of y1 are strongest predictors for future y1

```{r}
arima1 <- Arima(y = y1ts, order = c(5,0,4))
arima1
```

*Predictions with arima1

```{r}
arima1preds <- forecast(object = arima1, h = nrow(y1_test))

plot(arima1preds)
```


* Performance Metrics for arima1

```{r}
accuracy(arima1preds, y1_test$y1)

```

*Getting a MAPE of 154.59 with training set and 140.37 with test set.

##Now Arima with xregs (all columns)

```{r}
arima2 <- Arima(y = y1ts, order= c(5,0,4), xreg = subset(y1_train, select = -y1))
```

* Getting the error shown above.

###So Arima with all xregs in the dataset doesn't work. We will have to find out which columns to include.
For that, using StepAIC. IN that process, we will also evaluate the performance of OLS.

##StepAIC
###First with OLS
```{r}


base_linreg <- lm(y1 ~ ., data = y1_train)
base_linreg
```

###Predictions with OLS and its performance

```{r}

ols_all_preds <- predict(base_linreg, y1_test)

print('Error metrics for Train data')
print(regr.eval(ols_all_preds, y1_train$y1))
print("")
print('Error metrics for Test data')
print(regr.eval(ols_all_preds, y1_test$y1))
```

###OLS with all variables is giving a MAPE of 2.34 with Training data and 0.99 with Test Data!!!

* Let's see if StepAIC can do better

```{r}
library(MASS)
aicoptions <- stepAIC(base_linreg, direction = "both")
```


####Based on stepAIC, will use only the following regressors:

lm(formula = y1 ~ d_0 + d_1 + d_4 + f_16 + f_18 + f_20 + f_21 + 
    f_29 + f_34 + f_39 + f_40 + f_48 + f_49 + f_52 + f_58 + t_2 + 
    t_6 + t_24 + t_25 + t_34 + t_39 + t_43 + t_44 + f_33 + f_9 + 
    f_19 + t_14, data = y1_train)
    
    
```{r}
colnames(aicoptions$model)

myfactors <- colnames(aicoptions$model)
myfactors
myfactors <- myfactors[-1]
myfactors

#reformulate(termlabels = listoffactors, response = 'y')
lmformula <- reformulate(termlabels = myfactors, response = "y1")
lmformula
aic_rec_mdl <- lm(formula = lmformula, data = y1_train)


aic_rec_mdl
```

```{r}
aic_preds <- predict(aic_rec_mdl, y1_test)

print('Error metrics for Train data')
print(regr.eval(aic_preds, y1_train$y1))
print("")
print('Error metrics for Test data')
print(regr.eval(aic_preds, y1_test$y1))



```

* WIth StepAIC recommended model MAPE on training went up to 3.2 and on test went up to 1.09


##Now we will try Arima with regressors based on StepAIC recommended regressors

```{r}

new_y1_xreg <- subset(y1_train[, colnames(aic_rec_mdl$model)], select = -y1)

arima3 <- Arima(y = y1ts, order = c(5,0,4), xreg = new_y1_xreg)

arima3_preds <- forecast(object = arima3, h = nrow(y1_test), xreg = new_y1_xreg)
print(accuracy(arima3_preds,y1_test$y1))

plot(arima3_preds)


```

* Looks like regressors have made the predictions worse. MAPE is 1064.665 for training set and 354.393 for test set.


ACF and PACF of residuals

```{r}
acf(arima3$residuals)
```
```{r}
pacf(arima3$residuals)
```

#Trying with auto.arima

```{r}

arima4 <- auto.arima(y = y1ts, xreg = new_y1_xreg)


arima4

arima4_preds <- forecast(object = arima4, h = nrow(y1_test), xreg = new_y1_xreg)
print(accuracy(arima4_preds,y1_test$y1))

plot(arima4_preds)

```

Not much improvement in MAPE with AutoArima suggested (3,0,0)

##Seeing the performance of LASSO

###First will try a LASSO model with all variables

```{r}
library(glmnet)
set.seed(1234)

cv_lasso <- cv.glmnet(as.matrix(subset(y1_train, select = -y1)), as.matrix(y1_train$y1), alpha = 1, type.measure = "deviance", nfolds = 4)
par(mfrow=c(1,2))
plot(cv_lasso)
plot(cv_lasso$glmnet.fit, xvar = 'lambda', label = TRUE)
par(mfrow=c(1,1))


```

* Now using the min lambda, running lasso regression

```{r}
lasso_1 <- glmnet(x = as.matrix(subset(y1_train, select = -y1)), y = as.matrix(y1_train$y1), family = "gaussian", alpha = 1, lambda = cv_lasso$lambda.min)

lasso_1_preds <- predict(lasso_1, as.matrix(subset(y1_test, select = -y1)))
regr.eval(lasso_1_preds, y1_test$y1)


```

* This gives a MAPE of 9.38 on test data

###Now trying LASSO with the set of variables from StepAIC

```{r}
new_y1_xreg
X_train = as.matrix(new_y1_xreg)

X_test <- as.matrix(subset(y1_test[, colnames(aic_rec_mdl$model)], select = -y1))

cv_lasso_1 <- cv.glmnet(X_train, as.matrix(y1_train$y1), alpha = 1, type.measure = "deviance", nfolds = 4)

par(mfrow=c(1,2))
plot(cv_lasso_1)
plot(cv_lasso_1$glmnet.fit, xvar = 'lambda', label = TRUE)
par(mfrow=c(1,2))



```



```{r}
lasso_2 <- glmnet(X_train, as.matrix(y1_train$y1), family = "gaussian", lambda = cv_lasso_1$lambda.min, alpha = 1)

lasso_2_preds <- predict(lasso_2, X_test)

regr.eval(lasso_2_preds, y1_test$y1)
```

MAPE with this LASSO model gives a mape of 1.09


###refactor below







#ARIMA Revisited
###This is inspired by the discussion with Vamshi Anand (Batch 35). All credit
to him if this works out to be a better model

* The previous models with ARIMA were quite poorly performing. The response variable y1 is a daily percentage change. Hence the trend was quite linear and flat.

*The idea is what if we instead create a new response variable that is actually the price based on the price change. Can we see a trend and seasonality that would in turn give a better model?

* Let's create a new column DailyPrice that is an actual value that is previous day's value * percent change for the day.First day's price is set to 100 as a base.

```{r}
#function to help set the daily price.
setdailyprice <- function(daily_price) {
  retprice <- daily_price
  for(i in 1:length(daily_price)) {
    if (i == 1) {
      retprice[i] = 100
    } else {
      retprice[i] = retprice[i-1]*(1+ daily_price[i])
    }

  }
  return(retprice)
}

fin_train$DailyPrice <- setdailyprice(fin_data$y1)
head(fin_data$DailyPrice, 10)


```


###Train-Test sets with this new variable

```{r}
fin_data1 <- fin_data

fin_data1$DailyPrice <- setdailyprice(fin_data$y1)

train_rows <- 1:round(nrow(fin_data)*0.7)

fin_train1 <- fin_data[train_rows, ]

fin_test1 <- fin_data[-train_rows, ]

dp_train1 <- subset(fin_train1, select = c(-y1, -y2))
dp_test1 <- subset(fin_test1, select = c(-y1, -y2))


```


* Now we can create a new timeseries using this variable

```{r}

dailyprice_ts <- ts(fin_train1$DailyPrice, start = c(2010, 1,1), frequency = 252)
plot(dailyprice_ts)
```

```{r}
plot(decompose(dailyprice_ts))

```

*Let's see if we need to difference anything

```{r}
print(ndiffs(dailyprice_ts))
print(nsdiffs(dailyprice_ts))
```

We need to add one non-seasonal difference

ACF

```{r}
acf(dailyprice_ts, lag.max = 30)
```

ACF shows that it is a very slow decay and strong connect evey beyond 
PACF

```{r}
pacf(dailyprice_ts, lag.max = 30)
```

Will let auto arima pick the right values

* First just DailyPrice (ie no xregs)
```{r}

arima5 <- auto.arima(y = dailyprice_ts)
arima5

```

```{r}

arima5_preds <- forecast(arima5, h = nrow(y1_test))
plot(arima5_preds)
print(accuracy(arima5_preds, dp_test1$DailyPrice))


```

This model did very well with Training set (MAPE of 1.97) but not as good against the test set where the MAPE was 13.09

* How about the Regressors?

```{r}
arima6 <- auto.arima(dailyprice_ts, xreg = new_y1_xreg, allowdrift = TRUE)
arima6
```

Predictions and Performance
```{r}

arima6_preds <- forecast(arima6, h = nrow(new_y1_test), xreg = new_y1_xreg)
plot(arima6_preds)
accuracy(arima6_preds, dp_test1$DailyPrice)

```

